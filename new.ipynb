{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "94fc0379",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_24296\\3559934027.py:17: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df = df.replace('-', np.nan)\n",
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_24296\\3559934027.py:28: FutureWarning: errors='ignore' is deprecated and will raise in a future version. Use to_numeric without passing `errors` and catch exceptions explicitly instead\n",
      "  df[c] = pd.to_numeric(df[c], errors=\"ignore\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîé ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤ Hyperparameters ‡∏Ç‡∏≠‡∏á Random Forest...\n",
      "Fitting 3 folds for each of 20 candidates, totalling 60 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\model_selection\\_validation.py:516: FitFailedWarning: \n",
      "9 fits failed out of a total of 60.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "3 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\HP\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\model_selection\\_validation.py\", line 859, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\HP\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\base.py\", line 1365, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\HP\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\pipeline.py\", line 663, in fit\n",
      "    self._final_estimator.fit(Xt, y, **last_step_params[\"fit\"])\n",
      "  File \"C:\\Users\\HP\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\base.py\", line 1358, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"C:\\Users\\HP\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\base.py\", line 471, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"C:\\Users\\HP\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\utils\\_param_validation.py\", line 98, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'max_features' parameter of RandomForestRegressor must be an int in the range [1, inf), a float in the range (0.0, 1.0], a str among {'log2', 'sqrt'} or None. Got 'auto' instead.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "6 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\HP\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\model_selection\\_validation.py\", line 859, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\HP\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\base.py\", line 1365, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\HP\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\pipeline.py\", line 663, in fit\n",
      "    self._final_estimator.fit(Xt, y, **last_step_params[\"fit\"])\n",
      "  File \"C:\\Users\\HP\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\base.py\", line 1358, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"C:\\Users\\HP\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\base.py\", line 471, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"C:\\Users\\HP\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\utils\\_param_validation.py\", line 98, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'max_features' parameter of RandomForestRegressor must be an int in the range [1, inf), a float in the range (0.0, 1.0], a str among {'sqrt', 'log2'} or None. Got 'auto' instead.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "C:\\Users\\HP\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\model_selection\\_search.py:1135: UserWarning: One or more of the test scores are non-finite: [-16.78304905 -16.49259317 -16.9236057           nan -16.79402992\n",
      " -16.83424558 -16.62955252 -17.11029802 -16.97950982 -17.04285107\n",
      " -17.11969307 -16.77488899 -17.01135279          nan -16.49259317\n",
      " -17.12803412 -16.75527767 -16.54158394          nan -16.8090907 ]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF best params: {'model__n_estimators': 600, 'model__min_samples_split': 2, 'model__min_samples_leaf': 1, 'model__max_features': 'log2', 'model__max_depth': None}\n",
      "RF best CV score (RMSE): 16.492593165811712\n",
      "\n",
      "üîé ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤ Hyperparameters ‡∏Ç‡∏≠‡∏á XGBoost...\n",
      "Fitting 3 folds for each of 20 candidates, totalling 60 fits\n",
      "XGB best params: {'model__subsample': 0.9, 'model__reg_lambda': 0.5, 'model__n_estimators': 400, 'model__max_depth': 5, 'model__learning_rate': 0.01, 'model__colsample_bytree': 0.7}\n",
      "XGB best CV score (RMSE): 15.52607254452583\n",
      "\n",
      "Random Forest Test Metrics\n",
      "RMSE: 14.045687215364431\n",
      "MAE: 11.855822222222223\n",
      "R2 : 0.4518200776624157\n",
      "\n",
      "XGBoost Test Metrics\n",
      "RMSE: 12.167189548327931\n",
      "MAE: 9.639799466133118\n",
      "R2 : 0.5886441414176657\n",
      "\n",
      "‚úÖ ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ó‡∏µ‡πà‡πÄ‡∏•‡∏∑‡∏≠‡∏Å: XGBoost\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['model_tuned/best_model.pkl']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV, KFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "# -------------------- 1) ‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• --------------------\n",
    "path = Path(\"new data.xlsx\")\n",
    "df = pd.read_excel(path, sheet_name=\"Sheet2\")\n",
    "df = df.replace('-', np.nan)\n",
    "\n",
    "if \"Date\" in df.columns:\n",
    "    df[\"Date\"] = pd.to_datetime(df[\"Date\"], errors=\"coerce\")\n",
    "    df[\"Year\"] = df[\"Date\"].dt.year\n",
    "    df[\"Month\"] = df[\"Date\"].dt.month\n",
    "    df[\"Day\"] = df[\"Date\"].dt.day\n",
    "    df = df.drop(columns=[\"Date\"])\n",
    "\n",
    "for c in df.columns:\n",
    "    if c != \"dBZ\":\n",
    "        df[c] = pd.to_numeric(df[c], errors=\"ignore\")\n",
    "\n",
    "df = df[~df[\"dBZ\"].isna()].copy()\n",
    "y = df[\"dBZ\"]\n",
    "X = df.drop(columns=[\"dBZ\"])\n",
    "numeric_cols = X.select_dtypes(include=[np.number]).columns.tolist()\n",
    "X = X[numeric_cols]\n",
    "\n",
    "# -------------------- 2) Pipeline --------------------\n",
    "numeric_features = X.columns.tolist()\n",
    "preprocess = ColumnTransformer(\n",
    "    transformers=[(\"num\", Pipeline([(\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "                                     (\"scaler\", StandardScaler())]), numeric_features)],\n",
    "    remainder=\"drop\"\n",
    ")\n",
    "\n",
    "# -------------------- 3) Split Train/Test --------------------\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# -------------------- 4) Hyperparameter search space --------------------\n",
    "rf = Pipeline([(\"prep\", preprocess),\n",
    "               (\"model\", RandomForestRegressor(random_state=42))])\n",
    "\n",
    "xgb = Pipeline([(\"prep\", preprocess),\n",
    "                (\"model\", XGBRegressor(random_state=42, n_jobs=2, objective=\"reg:squarederror\"))])\n",
    "\n",
    "param_dist_rf = {\n",
    "    \"model__n_estimators\": [100, 200, 400, 600],\n",
    "    \"model__max_depth\": [None, 5, 10, 20],\n",
    "    \"model__min_samples_split\": [2, 5, 10],\n",
    "    \"model__min_samples_leaf\": [1, 2, 4],\n",
    "    \"model__max_features\": [\"auto\", \"sqrt\", \"log2\"]\n",
    "}\n",
    "\n",
    "param_dist_xgb = {\n",
    "    \"model__n_estimators\": [100, 200, 400],\n",
    "    \"model__max_depth\": [3, 4, 5, 6],\n",
    "    \"model__learning_rate\": [0.01, 0.05, 0.1, 0.2],\n",
    "    \"model__subsample\": [0.7, 0.8, 0.9, 1.0],\n",
    "    \"model__colsample_bytree\": [0.7, 0.8, 0.9, 1.0],\n",
    "    \"model__reg_lambda\": [0.5, 1.0, 1.5, 2.0]\n",
    "}\n",
    "\n",
    "# -------------------- 5) RandomizedSearchCV --------------------\n",
    "cv = KFold(n_splits=3, shuffle=True, random_state=42)\n",
    "\n",
    "search_rf = RandomizedSearchCV(rf, param_dist_rf, n_iter=20, scoring=\"neg_root_mean_squared_error\",\n",
    "                               n_jobs=-1, cv=cv, verbose=1, random_state=42)\n",
    "search_xgb = RandomizedSearchCV(xgb, param_dist_xgb, n_iter=20, scoring=\"neg_root_mean_squared_error\",\n",
    "                                n_jobs=-1, cv=cv, verbose=1, random_state=42)\n",
    "\n",
    "print(\"üîé ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤ Hyperparameters ‡∏Ç‡∏≠‡∏á Random Forest...\")\n",
    "search_rf.fit(X_train, y_train)\n",
    "print(\"RF best params:\", search_rf.best_params_)\n",
    "print(\"RF best CV score (RMSE):\", -search_rf.best_score_)\n",
    "\n",
    "print(\"\\nüîé ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤ Hyperparameters ‡∏Ç‡∏≠‡∏á XGBoost...\")\n",
    "search_xgb.fit(X_train, y_train)\n",
    "print(\"XGB best params:\", search_xgb.best_params_)\n",
    "print(\"XGB best CV score (RMSE):\", -search_xgb.best_score_)\n",
    "\n",
    "# -------------------- 6) ‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏ö‡∏ô Test set --------------------\n",
    "best_rf = search_rf.best_estimator_\n",
    "best_xgb = search_xgb.best_estimator_\n",
    "\n",
    "for name, model in [(\"Random Forest\", best_rf), (\"XGBoost\", best_xgb)]:\n",
    "    y_pred = model.predict(X_test)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    print(f\"\\n{name} Test Metrics\")\n",
    "    print(\"RMSE:\", rmse)\n",
    "    print(\"MAE:\", mae)\n",
    "    print(\"R2 :\", r2)\n",
    "\n",
    "# -------------------- 7) ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ó‡∏µ‡πà‡∏î‡∏µ‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î --------------------\n",
    "final_model = best_rf if r2_score(y_test, best_rf.predict(X_test)) > r2_score(y_test, best_xgb.predict(X_test)) else best_xgb\n",
    "print(\"\\n‚úÖ ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ó‡∏µ‡πà‡πÄ‡∏•‡∏∑‡∏≠‡∏Å:\", \"Random Forest\" if final_model == best_rf else \"XGBoost\")\n",
    "\n",
    "# -------------------- 8) ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÇ‡∏°‡πÄ‡∏î‡∏• --------------------\n",
    "import joblib\n",
    "Path(\"model_tuned\").mkdir(exist_ok=True)\n",
    "joblib.dump(final_model, \"model_tuned/best_model.pkl\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
